# https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/values.yaml
# for chart: https://github.com/grafana/mimir/tree/main/operations/helm/charts/mimir-distributed

nodePools:
  controlPlane: &controlPlaneSelector
    node-role.kubernetes.io/control-plane: "true"
  logsIngest: &logsIngestSelector
    logs: "ingest"
  metricsHot: &metricsHotSelector
    metrics: "hot"
  tracesHot: &tracesHotSelector
    traces: "hot"
  queryCpu: &queryCpuSelector
    query: "cpu"
  general: &generalSelector
    general: "true"
  tierMinio: &minioSelector
    tier: "minio"

mimir:
  image:
    tag: "latest"

  structuredConfig:
    # multitenancy_enabled: true # Is enabled by default.
    query_scheduler: # to prevent "too many outstanding requests" when using query_scheduler
      max_outstanding_requests_per_tenant: 10000
    #frontend: # to prevent "too many outstanding requests" when using query_frontend
    #  max_outstanding_per_tenant: 10000
    common:
      storage:
        backend: s3
        s3:
          bucket_name: mimir
          endpoint: minio-tenant-hl.minio-tenant.svc.cluster.local:9000
          region: minio
          access_key_id: minio
          secret_access_key: minio123
          insecure: false
          http:
            insecure_skip_verify: true
    limits:
      # OTEL tweaks to get service.name as label in Mimir (grealty helps with Grafana Explore/Drilldown):
      otel_keep_identifying_resource_attributes: true
      promote_otel_resource_attributes: "service.instance.id, service.name, service.namespace, service.version, cloud.availability_zone, cloud.region, container.name, deployment.environment, deployment.environment.name, k8s.cluster.name, k8s.container.name, k8s.cronjob.name, k8s.daemonset.name, k8s.deployment.name, k8s.job.name, k8s.namespace.name, k8s.pod.name, k8s.replicaset.name, k8s.statefulset.name"
      ingestion_rate: 100000        # samples/sec
      ingestion_burst_size: 2000000 # samples burst
    blocks_storage:
      s3:
        bucket_name: mimir-blocks
    alertmanager_storage:
      s3:
        bucket_name: mimir-alerts
    ruler_storage:
      s3:
        bucket_name: mimir-ruler

minio:
  enabled: false
  
distributor:
  replicas: 2
  #nodeSelector: *metricsHotSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "1"
      memory: "1Gi"

ingester:
  zoneAwareReplication:
    enabled: false
  replicas: 3
  #nodeSelector: *metricsHotSelector
  resources:
    requests:
      cpu: "100m"
      memory: "2Gi"
    limits:
      cpu: "1"
      memory: "3Gi"
  persistentVolume:
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 10Gi

querier:
  replicas: 3
  #nodeSelector: *queryCpuSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "1000m"
      memory: "512Mi"

query_frontend:
  replicas: 2
  #nodeSelector: *queryCpuSelector
  config:
    multitenancy_enabled: true
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"

store_gateway:
  replicas: 3
  resources:
    requests:
      cpu: "1000m"
      memory: "1Gi"
    limits:
      cpu: "2000m"
      memory: "2Gi"
  persistence:
    enabled: false
    size: 50Gi   # on local nVME storage
  extraArgs:
    - -store-gateway.sharding-enabled=true
    - -blocks-storage.bucket-store.index-header.lazy-loading-idle-timeout=5m

compactor:
  replicas: 1
  #nodeSelector: *metricsHotSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"

alertmanager:
  #nodeSelector: *metricsHotSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"

nginx:
  enabled: true
  replicas: 1
  #nodeSelector: *queryCpuSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"

query_scheduler:
  replicas: 1
  #nodeSelector: *queryCpuSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"

ruler:
  replicas: 1
  #nodeSelector: *metricsHotSelector
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "256Mi"
  readinessProbe:
    httpGet:
      path: /ready
      port: 8080
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
  podDisruptionBudget:
    maxUnavailable: 1
overrides_exporter:
  enabled: true
  replicas: 1
  #nodeSelector: *generalSelector